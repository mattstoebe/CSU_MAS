# -*- coding: utf-8 -*-
"""02_Model_Exploration

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KGCYf0MbtcvS9sWFbSZ4EMclFK2B_EXx
"""

"""# Full Process

## Utils
"""

from copy import deepcopy
from sklearn.metrics import precision_recall_curve, f1_score, classification_report, confusion_matrix
from sklearn.model_selection  import train_test_split, GridSearchCV, StratifiedKFold, RandomizedSearchCV
from scipy.stats import randint, uniform
import xgboost as xgb
from sklearn.impute import KNNImputer
import matplotlib.pyplot as plt
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
import shap
import joblib

"""## Load Data"""

import pandas as pd
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix

# Set pandas display options to prevent truncation
pd.set_option('display.max_columns', None)
pd.set_option('display.width', None)

# Define target and feature treatments
target = 'ptp3_yn'

feature_treatments = {
      'age': 'continuous',
      'race7': 'factor_nominal',
      'ethnic3': 'factor_nominal',
      'sex2': 'factor_nominal',
      'sig_other': 'factor_nominal',
      'tobacco': 'factor_nominal',
      'alcohol': 'factor_nominal',
      'drugs': 'factor_nominal',
      'MedianIncomeForZip': 'continuous',
      'PercentAboveHighSchoolEducationForZip': 'continuous',
      'PercentAboveBachelorsEducationForZip': 'continuous',
      'payertype': 'factor_nominal',
      'tbiS02': 'continuous',
      'tbiS06': 'continuous',
      'tbiS09': 'continuous',
      'ptp1_yn': 'factor_nominal',
      'ptp2_yn': 'factor_nominal',
      'ptp0_yn': 'factor_nominal',
      'ed_yn': 'factor_nominal',
      'icu': 'factor_nominal',
      'delirium': 'factor_nominal',
      'agitated': 'factor_nominal',
      'lethargic': 'factor_nominal',
      'comatose': 'factor_nominal',
      'disoriented': 'factor_nominal',
      'gcs_min': 'continuous',
      'gcs_max': 'continuous',
      'adl_min': 'continuous',
      'adl_max': 'continuous',
      'mobility_min': 'continuous',
      'mobility_max': 'continuous',
      'los_total': 'continuous',
      'dc_setting': 'factor_nominal',
      'prehosp': 'factor_nominal',
      'posthosp': 'factor_nominal',
}

dtype_mapping = {
    'continuous': 'float64',  # Use float64 for continuous data
    'factor_nominal': 'category' # Use category for nominal factors
}
dtype_dict = {col: dtype_mapping[treatment] for col, treatment in feature_treatments.items()}
dtype_dict['subj_id'] = 'object'
dtype_dict['ptp3_yn'] = 'int64'
dtype_dict['died'] = 'int64'


nominal_cols = [col for col, treatment in feature_treatments.items() if treatment == 'factor_nominal']
continuous_cols = [col for col, treatment in feature_treatments.items() if treatment == 'continuous']

path = r'/Users/mattstoebe/ClonedRepos/CSU_MAS_/Capstone/tbistaa556_training.csv'


df = pd.read_csv(path, dtype=dtype_dict, na_values=[' '])
df = pd.get_dummies(df, columns=nominal_cols, drop_first=True)
features = [col for col in df.columns if col not in ['ptp3_yn', 'died', 'subj_id']]

# print(features)
print(df.info())

X = df[features]
y = df[target]

X_tr, X_tmp, y_tr, y_tmp = train_test_split(
    X, y, test_size=0.30, stratify=y, random_state=42)

X_val, X_te, y_val, y_te = train_test_split(
    X_tmp, y_tmp, test_size=0.66, stratify=y_tmp, random_state=42)

print(f"Train: {X_tr.shape}  |  Val: {X_val.shape}  |  Test: {X_te.shape}")

scale_pos = (y_tr == 0).sum() / (y_tr == 1).sum()

pipe = make_pipeline(
    StandardScaler(),
    xgb.XGBClassifier(
        objective          = 'binary:logistic',
        eval_metric        = 'aucpr',
        use_label_encoder  = False,
        scale_pos_weight   = scale_pos,
        tree_method        = 'hist',
        n_jobs             = -1,
        random_state       = 42)
)

# Define search space
param_dist = {
    # tree complexity
    'xgbclassifier__max_depth'       : randint(2, 9),
    'xgbclassifier__min_child_weight': randint(1, 11),

    # learning-rate / trees trade-off
    'xgbclassifier__learning_rate'   : uniform(0.02, 0.18),
    'xgbclassifier__n_estimators'    : randint(300, 1001),

    # regularisation
    'xgbclassifier__gamma'           : uniform(0, 1.0),
    'xgbclassifier__lambda'          : uniform(0.5, 9.5),
    'xgbclassifier__alpha'           : uniform(0, 2.0),

    # sampling
    'xgbclassifier__subsample'       : uniform(0.7, 0.3),
    'xgbclassifier__colsample_bytree': uniform(0.4, 0.6)
}


# Random Search over larger search space
rs = RandomizedSearchCV(
        estimator   = pipe,
        param_distributions = param_dist,
        n_iter      = 80,
        scoring     = 'average_precision',
        cv          = StratifiedKFold(n_splits=3, shuffle=True, random_state=42),
        n_jobs      = -1,
        verbose     = 1,
        random_state= 42)

rs.fit(X_tr, y_tr)
best_model  = rs.best_estimator_
best_params = rs.best_params_

# -- 3. threshold tuning
val_prob = best_model.predict_proba(X_val)[:, 1]
prec_v, rec_v, thr = precision_recall_curve(y_val, val_prob)
f1s   = [f1_score(y_val, val_prob > t) for t in thr]
tau   = thr[int(max(range(len(f1s)), key=f1s.__getitem__))]
val_f1 = max(f1s)

# -- 4. final test
test_prob = best_model.predict_proba(X_te)[:, 1]
test_pred = (test_prob > tau).astype(int)

print("\nBEST PARAMS:", best_params)
print(f"Chosen Ï„ = {tau:.3f}  |  F1 on val = {val_f1:.3f}\n")
print(classification_report(y_te, test_pred, digits=3))
print("Confusion matrix:\n", confusion_matrix(y_te, test_pred))

prec_te, rec_te, _ = precision_recall_curve(y_te, test_prob)
results = {
    'model'      : best_model,
    'tau'        : tau,
    'best_params': best_params,
    'val_f1'     : val_f1,
    'pr_curve_val': (rec_v, prec_v),
    'pr_curve_te' : (rec_te, prec_te)
}

"""## Gridsearch and train model"""

plt.figure(figsize=(10, 6))
plt.plot(thr, f1s, marker='.', label='F1 Score')

plt.plot(thr, prec[:-1], marker='.', label='Precision')
plt.plot(thr, rec[:-1], marker='.', label='Recall')

plt.title(f'F1 Score, Precision, and Recall vs. Threshold (Validation Set)')
plt.xlabel('Threshold')
plt.ylabel('Score')
plt.legend()
plt.grid(True)
plt.show()

print("\nFitting final model on all data with best parameters...")

# Fit the final pipeline on the entire dataset
pipe.fit(X, y)

print("Final model fitted on all data.")


pipe.fit(X, y)

X_proc  = pipe[:-1].transform(X)
if hasattr(X_proc, "toarray"):
    X_proc = X_proc.toarray()


# Create a SHAP explainer object
explainer = shap.Explainer(pipe['xgbclassifier'], X_proc)

# Generate SHAP values for the test data
shap_values = explainer(X)

# Visualize feature importance
shap.summary_plot(shap_values, X)

joblib.dump(pipe, 'model.joblib')