---
title: "551 HW 3"
author: "Matthew Stoebe"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

install.packages('broom')
```

#10.6:

Regression models with interactions: The folder Beauty contains data (use file beauty.csv) from Hamermesh and Parker (2005) on student evaluations of instructors’ beauty and teaching quality for several courses at the University of Texas. The teaching evaluations were conducted at the end of the semester, and the beauty judgments were made later, by six students who had not attended the classes and were not aware of the course evaluations.
(a) Run a regression using beauty (the variable beauty) to predict course evaluations (eval), adjusting for various other predictors. Graph the data and fitted model, and explain the meaning of each of the coefficients along with the residual standard deviation. Plot the residuals versus fitted values.

(b) Fit some other models, including beauty and also other predictors. Consider at least one model with interactions. For each model, explain the meaning of each of its estimated
coefficients.


```{r}
# Load necessary libraries
library(ggplot2)
library(dplyr)
library(broom) # for tidy model summaries

# Load the data
df <- read.csv("./Other Data/HW_3/beauty.csv")

# Inspect the first few rows of the data
head(df)

# Model 1: Simple linear regression with beauty and other predictors
model1 <- lm(eval ~ beauty + age + female + minority + nonenglish, data = df)

# Model 2: Adding an interaction term between beauty and female
model2 <- lm(eval ~ beauty * female + age + minority + nonenglish, data = df)

# Model 3: Adding interactions between beauty and other variables
model3 <- lm(eval ~ beauty * female + beauty * age + beauty * minority + beauty * nonenglish, data = df)


# Summary of each model
summary(model1)
summary(model2)
summary(model3)

# Create a table comparing the coefficients of each model using broom's tidy() function
coef_model1 <- tidy(model1) %>% mutate(model = "Model 1")
coef_model2 <- tidy(model2) %>% mutate(model = "Model 2")
coef_model3 <- tidy(model3) %>% mutate(model = "Model 3")

# Combine the coefficients from all models
coef_comparison <- bind_rows(coef_model1, coef_model2, coef_model3)


coef_combined <- coef_comparison %>% filter(term != "(Intercept)")


# Print the comparison of coefficients across models
coef_comparison %>%
  select(term, estimate, std.error, statistic, p.value, model) %>%
  arrange(term, model) %>%
  print()

# Plot residuals vs fitted values for each model

# Model 1 Residuals vs Fitted
residuals1 <- resid(model1)
fitted_values1 <- fitted(model1)

ggplot(data.frame(fitted = fitted_values1, residuals = residuals1), aes(x = fitted, y = residuals)) +
  geom_point() +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  labs(title = "Residuals vs Fitted Values (Model 1)", x = "Fitted Values", y = "Residuals")

# Model 2 Residuals vs Fitted
residuals2 <- resid(model2)
fitted_values2 <- fitted(model2)

ggplot(data.frame(fitted = fitted_values2, residuals = residuals2), aes(x = fitted, y = residuals)) +
  geom_point() +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  labs(title = "Residuals vs Fitted Values (Model 2)", x = "Fitted Values", y = "Residuals")

# Model 3 Residuals vs Fitted
residuals3 <- resid(model3)
fitted_values3 <- fitted(model3)

ggplot(data.frame(fitted = fitted_values3, residuals = residuals3), aes(x = fitted, y = residuals)) +
  geom_point() +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  labs(title = "Residuals vs Fitted Values (Model 3)", x = "Fitted Values", y = "Residuals")


ggplot(coef_combined, aes(x = term, y = estimate, fill = model)) +
  geom_bar(stat = "identity", position = position_dodge()) +
  labs(title = "Comparison of Coefficients Across Models", 
       x = "Predictors", 
       y = "Coefficient Estimate (Slope)") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```
## a: Coefficient and residual standard deviation Interpretation
1. Beauty: 0.141. For every unit increase in the beauty rating, the course evaluation is expected to increase by approximately 0.141, holding all other variables constant. This indicates a positive effect of perceived beauty on evaluations.
2. Age: -0.0027. There is a slight negative association between age and course evaluation, but the effect is small and not statistically significant.
3. Female: -0.207. Female instructors receive evaluations that are, on average, 0.207 points lower than male instructors, holding other factors constant. This result is statistically significant.
4, Minority: -0.044. Minority instructors receive evaluations that are 0.044 points lower than non-minority instructors, but this effect is not statistically significant.
5. Non-English: -0.313. Instructors who are non-native English speakers tend to receive evaluations that are 0.313 points lower than native English speakers, a statistically significant result.

Residual Standard Deviation: 0.532. This indicates the typical deviation of observed evaluations from the predicted values by the model.

## b: Interpretation of coefficients for other models including interactions
### Model 2: 
1. Beauty: 0.194. The effect of beauty on course evaluations for male instructors (reference group) is a positive increase of 0.194 per unit increase in beauty.
2. Female: -0.214. Female instructors receive evaluations that are 0.214 points lower than male instructors when beauty is held constant.
3. Female:Beauty : -0.111. This indicates that for female instructors, the effect of beauty is smaller. In particular, for every unit increase in beauty, female instructors receive an increase of 0.111 points less compared to male instructors. This interaction is marginally significant.
4. Other coefficients remain similar to the first model.

### Model 3: 
1. Beauty: -0.443. In this model, the direct effect of beauty is negative. However, its interactions with other variables need to be considered for interpretation.
2. female:Beauty: 0.0125. This suggests that the effect of beauty increases with age. Older instructors benefit more from beauty in their evaluations.
3. minority:Beauty: -0.139. The negative interaction indicates that for minority instructors, beauty has a smaller positive effect on evaluations.
4. Beauty:nonenglish: 0.286. Non-native English speakers benefit more from beauty in their evaluations.

Residual standard error: 0.524. This model has a slightly lower residual standard error, indicating a better fit than the previous two models.

#11.5

Residuals and predictions: The folder Pyth contains outcome y and predictors x1, x2 for 40 data points, with a further 20 points with the predictors but no observed outcome. Save the file to your working directory, then read it into R using read.table().
(a) Use R to fit a linear regression model predicting y from x1, x2, using the first 40 data points in the file. Summarize the inferences and check the fit of your model.
(b) Display the estimated model graphically as in Figure 11.2.
(c) Make a residual plot for this model. Do the assumptions appear to be met?
(d) Make predictions for the remaining 20 data points in the file. How confident do you feel about these predictions?

```{r}
# (a) Read the data with proper column names
data <- read.table("./Other Data/HW_3/pyth.txt", header = TRUE)

# Convert columns to numeric
data$x1 <- as.numeric(data$x1)
data$x2 <- as.numeric(data$x2)
data$y <- as.numeric(data$y)

# Separate the training data (first 40 rows) and test data (remaining 20 rows without 'y')
train_data <- data[1:40, ]
test_data <- data[41:60, c("x1", "x2")]

# (a) Fit the linear regression model using the first 40 data points
model <- lm(y ~ x1 + x2, data = train_data)

# Summary of the model
summary(model)

# (b) Display the estimated model graphically for 'x1' with regression line
ggplot(train_data, aes(x = x1, y = y)) +
  geom_point() +
  geom_abline(intercept = coef(model)[1] + coef(model)[3] * mean(train_data$x2), slope = coef(model)[2], color = "blue") +  # Add regression line
  labs(title = "Linear Regression of y on x1", x = "x1", y = "y") +
  theme_minimal()

# (b) Display the estimated model graphically for 'x2' with regression line
ggplot(train_data, aes(x = x2, y = y)) +
  geom_point() +
  geom_abline(intercept = coef(model)[1] + coef(model)[2] * mean(train_data$x1), slope = coef(model)[3], color = "green") +  # Add regression line
  labs(title = "Linear Regression of y on x2", x = "x2", y = "y") +
  theme_minimal()

# (c) Residual plot: plot residuals against fitted values
residuals <- resid(model)
fitted_values <- fitted(model)

ggplot(data.frame(fitted = fitted_values, residuals = residuals), aes(x = fitted, y = residuals)) +
  geom_point() +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  labs(title = "Residual Plot", x = "Fitted Values", y = "Residuals") +
  theme_minimal()

# (d) Predictions for the remaining 20 data points (test_data)
predictions <- predict(model, newdata = test_data)

# Print the predictions
print(predictions)


```

#12.6
12.6 Logarithmic transformations: The folder Pollution contains mortality rates and various
environmental factors from 60 U.S. metropolitan areas (see McDonald and Schwing, 1973). For
this exercise we shall model mortality rate given nitric oxides, sulfur dioxide, and hydrocarbons
as inputs. This model is an extreme oversimplification, as it combines all sources of mortality
and does not adjust for crucial factors such as age and smoking. We use it to illustrate log
transformations in regression.
(a) Create a scatterplot of mortality rate versus level of nitric oxides. Do you think linear regression will fit these data well? Fit the regression and evaluate a residual plot from the regression.
(b) Find an appropriate transformation that will result in data more appropriate for linear regression. Fit a regression to the transformed data and evaluate the new residual plot.
(c) Interpret the slope coefficient from the model you chose in (b)
(d) Now fit a model predicting mortality rate using levels of nitric oxides, sulfur dioxide, and hydrocarbons as inputs. Use appropriate transformations when helpful. Plot the fitted regression model and interpret the coefficients.
(e) Cross validate: fit the model you chose above to the first half of the data and then predict for the second half. You used all the data to construct the model in (d), so this is not really cross validation, but it gives a sense of how the steps of cross validation can be implemented.

```{r}
# Load necessary libraries
library(ggplot2)
library(dplyr)

# ===============================
# Load and Inspect the Dataset
# ===============================

# Define column names
column_names <- c("prec", "jant", "jult", "ovr65", "popn", "educ", "hous", "dens", 
                  "nonw", "wwdrk", "poor", "hc", "nox", "so2", "humid", "mort")

# Load the dataset
data <- read.csv("./Other Data/HW_3/pollution.csv", header = TRUE, col.names = column_names)

# Inspect the first few rows of the dataset
head(data)

# ===============================
# Part (a): Scatterplot and Linear Regression
# ===============================

# Create scatterplot of mortality rate vs nitric oxides (nox)
ggplot(data, aes(x = nox, y = mort)) +
  geom_point(color = "darkblue") +
  labs(title = "Mortality Rate vs Nitric Oxides (NOx)",
       x = "Nitric Oxides (NOx)",
       y = "Mortality Rate") +
  theme_minimal()

# Interpretation:
# - Observe the scatterplot to assess the relationship between NOx and mortality rate.
# - Look for patterns, such as linearity, curvature, or heteroscedasticity.

# Fit a simple linear regression model: mort ~ nox
linear_model <- lm(mort ~ nox, data = data)

# Summary of the linear regression model
summary(linear_model)

# Residuals vs Fitted Values Plot
ggplot(data, aes(x = fitted(linear_model), y = resid(linear_model))) +
  geom_point(color = "darkgreen") +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  labs(title = "Residuals vs Fitted Values (Linear Model)",
       x = "Fitted Values",
       y = "Residuals") +
  theme_minimal()

# Interpretation:
# - Check for randomness in residuals.
# - Patterns indicate potential model inadequacies (e.g., non-linearity).

# ===============================
# Part (b): Logarithmic Transformation and Regression
# ===============================

# Check for zero or negative values before log transformation
if(any(data$mort <= 0) | any(data$nox <= 0)) {
  stop("Mortality rate and NOx levels must be positive for log transformation.")
}

# Apply logarithmic transformation to both response and predictor
data <- data %>%
  mutate(log_mort = log(mort),
         log_nox = log(nox))

# Create scatterplot of log-transformed variables
ggplot(data, aes(x = log_nox, y = log_mort)) +
  geom_point(color = "purple") +
  labs(title = "Log-Log Scatterplot: Mortality Rate vs NOx",
       x = "Log Nitric Oxides (log NOx)",
       y = "Log Mortality Rate (log Mort)") +
  theme_minimal()

# Fit a linear regression model on log-transformed data
log_linear_model <- lm(log_mort ~ log_nox, data = data)

# Summary of the log-log regression model
summary(log_linear_model)

# Residuals vs Fitted Values Plot for Log-Transformed Model
ggplot(data, aes(x = fitted(log_linear_model), y = resid(log_linear_model))) +
  geom_point(color = "orange") +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  labs(title = "Residuals vs Fitted Values (Log-Log Model)",
       x = "Fitted Values (log scale)",
       y = "Residuals") +
  theme_minimal()

# Interpretation:
# - Assess improvement in residual patterns after transformation.
# - Reduced heteroscedasticity and better linearity indicate a better fit.

# ===============================
# Part (c): Interpretation of the Slope Coefficient
# ===============================

# Extract slope coefficient
slope_coef <- coef(log_linear_model)["log_nox"]

# Interpretation statement
cat(sprintf("Interpretation of Slope Coefficient:\nA 1%% increase in nitric oxides (NOx) level is associated with approximately %.2f%% increase in mortality rate, holding other factors constant.\n", 
            slope_coef * 100))

# ===============================
# Part (d): Multiple Regression with Transformed Data
# ===============================

# Check for zero or negative values in additional predictors
if(any(data$so2 <= 0) | any(data$hc <= 0)) {
  stop("SO2 and HC levels must be positive for log transformation.")
}

# Apply logarithmic transformation to additional predictors
data <- data %>%
  mutate(log_so2 = log(so2),
         log_hc = log(hc))

# Fit multiple linear regression model on log-transformed data
multi_log_model <- lm(log_mort ~ log_nox + log_so2 + log_hc, data = data)

# Summary of the multiple regression model
summary(multi_log_model)

# Residuals vs Fitted Values Plot for Multiple Log-Transformed Model
ggplot(data, aes(x = fitted(multi_log_model), y = resid(multi_log_model))) +
  geom_point(color = "brown") +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  labs(title = "Residuals vs Fitted Values (Multiple Log-Log Model)",
       x = "Fitted Values (log scale)",
       y = "Residuals") +
  theme_minimal()

# Interpretation:
# - Each coefficient represents the percentage change in mortality rate for a 1% change in the predictor, holding other variables constant.
# - Assess the significance and magnitude of each predictor.

# Optional: Visualize the relationship for each predictor separately
# Note: Visualizing multiple predictors simultaneously is complex; consider partial plots or other diagnostic tools.

# ===============================
# Part (e): Cross-Validation
# ===============================

# Split the data into first half and second half
set.seed(123) # For reproducibility
n <- nrow(data)
half <- floor(n / 2)

# Ensure data is shuffled before splitting to avoid any ordering bias
data_shuffled <- data %>% sample_frac(1)

first_half <- data_shuffled[1:half, ]
second_half <- data_shuffled[(half + 1):n, ]

# Fit the multiple regression model on the first half
cv_model <- lm(log_mort ~ log_nox + log_so2 + log_hc, data = first_half)

# Predict on the second half
cv_predictions_log <- predict(cv_model, newdata = second_half)

# Convert predictions back to the original scale
cv_predictions <- exp(cv_predictions_log)

# Actual mortality rates in the second half
actual_mort <- second_half$mort

# Calculate prediction errors (e.g., Mean Absolute Error)
mae <- mean(abs(cv_predictions - actual_mort))
rmse <- sqrt(mean((cv_predictions - actual_mort)^2))

# Output prediction results and error metrics
cat("Cross-Validation Predictions:\n")
print(data.frame(Actual = actual_mort, Predicted = cv_predictions))

cat(sprintf("\nPrediction Error Metrics:\nMean Absolute Error (MAE): %.2f\nRoot Mean Squared Error (RMSE): %.2f\n", mae, rmse))

# Optional: Plot Actual vs Predicted Mortality Rates
ggplot(data.frame(Actual = actual_mort, Predicted = cv_predictions), aes(x = Actual, y = Predicted)) +
  geom_point(color = "steelblue") +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red") +
  labs(title = "Actual vs Predicted Mortality Rates",
       x = "Actual Mortality Rate",
       y = "Predicted Mortality Rate") +
  theme_minimal()

```


#12.15
```{r}
# Load libraries
library(ggplot2)
library(dplyr)
library(rstanarm)
library(loo)
library(bayesplot)

# Set seed for reproducibility
set.seed(123)

# Define the path to the dataset
data_path <- "./Other Data/HW_3/portugese_language.csv"  # Ensure the correct path and filename

# Load the dataset
data <- read.csv(data_path, header = TRUE)

# Inspect the first few rows
head(data)

# Check the structure of the dataset
str(data)

# Define the predictors as per Section 12.7
predictors <- c("school", "sex", "age", "address", "famsize", "Pstatus",
               "Medu", "Fedu", "traveltime", "studytime", "failures",
               "schoolsup", "famsup", "paid", "activities", "nursery",
               "higher", "internet", "romantic", "famrel", "freetime",
               "goout", "Dalc", "Walc", "health", "absences")

# Define the outcome variable
outcome <- "G3port"  # Assuming the Portuguese grade column is named 'G3port'

# Subset the data to include only relevant columns and remove rows with missing values
data_G3port <- data %>%
  select(all_of(c(outcome, predictors))) %>%
  filter(!is.na(.data[[outcome]]))  # Remove rows where outcome is NA

# Check for missing values in predictors
sum(is.na(data_G3port))

# List of categorical predictors
categorical_vars <- c("school", "sex", "address", "famsize", "Pstatus",
                      "schoolsup", "famsup", "paid", "activities", "nursery",
                      "higher", "internet", "romantic")

# Convert categorical variables to factors
data_G3port[categorical_vars] <- lapply(data_G3port[categorical_vars], as.factor)

# Inspect the data structure after conversion
str(data_G3port)

# Check for non-positive values in outcome and predictors
if(any(data_G3port[[outcome]] <= 0)){
  stop("Outcome variable contains non-positive values. Log transformation is not possible.")
}

# Similarly, check for predictors if you plan to log-transform any
# For this analysis, we'll follow the original Section 12.7 approach without log transformations

# Fit a Bayesian linear regression model without standardizing predictors
# Using default priors
fit0 <- stan_glm(as.formula(paste(outcome, "~ .")),
                 data = data_G3port,
                 family = gaussian(),
                 prior = normal(0, 2.5),
                 prior_intercept = normal(0, 5),
                 chains = 4,
                 iter = 2000,
                 seed = 123)

# Summarize the model
summary(fit0)

# Standardize the predictors (mean = 0, SD = 1)
datastd_G3port <- data_G3port

# Standardize numerical predictors
numeric_vars <- predictors[!(predictors %in% categorical_vars)]
datastd_G3port[numeric_vars] <- scale(datastd_G3port[numeric_vars])

# Fit the Bayesian linear regression model with standardized predictors
fit1 <- stan_glm(as.formula(paste(outcome, "~ .")),
                data = datastd_G3port,
                family = gaussian(),
                prior = normal(0, 2.5),
                prior_intercept = normal(0, 5),
                chains = 4,
                iter = 2000,
                seed = 123)

# Summarize the model
summary(fit1)

# Define the number of predictors
p <- length(predictors)
n <- nrow(datastd_G3port)

# Assume a prior guess that the proportion of explained variance is near 0.3
# Calculate the prior scale for coefficients
prior_scale <- sd(datastd_G3port[[outcome]]) * sqrt(0.3 / p)

# Fit the model with scaled normal priors
fit2 <- stan_glm(as.formula(paste(outcome, "~ .")),
                data = datastd_G3port,
                family = gaussian(),
                prior = normal(0, prior_scale),
                prior_intercept = normal(0, 5),
                chains = 4,
                iter = 2000,
                seed = 123)

# Summarize the model
summary(fit2)

# Define parameters for the regularized horseshoe prior
p <- length(predictors)
n <- nrow(datastd_G3port)
p0 <- 6  # Expected number of relevant predictors

# Calculate slab scale
slab_scale <- sqrt(0.3 / p0) * sd(datastd_G3port[[outcome]])

# Calculate global scale
global_scale <- (p0 / (p - p0)) / sqrt(n)

# Fit the model with regularized horseshoe prior
fit3 <- stan_glm(as.formula(paste(outcome, "~ .")),
                data = datastd_G3port,
                family = gaussian(),
                prior = hs(global_scale = global_scale, slab_scale = slab_scale),
                prior_intercept = normal(0, 5),
                chains = 4,
                iter = 2000,
                seed = 123)

# Summarize the model
summary(fit3)

# Calculate Bayesian R^2 for all models
bayes_R2_fit0 <- bayes_R2(fit0)
bayes_R2_fit1 <- bayes_R2(fit1)
bayes_R2_fit2 <- bayes_R2(fit2)
bayes_R2_fit3 <- bayes_R2(fit3)

# Display Bayesian R^2
print(bayes_R2_fit0)
print(bayes_R2_fit1)
print(bayes_R2_fit2)
print(bayes_R2_fit3)

# Calculate LOO for all models
loo_fit0 <- loo(fit0)
loo_fit1 <- loo(fit1)
loo_fit2 <- loo(fit2)
loo_fit3 <- loo(fit3)

# Extract LOO R^2
loo_R2_fit0 <- loo_R2(loo_fit0)
loo_R2_fit1 <- loo_R2(loo_fit1)
loo_R2_fit2 <- loo_R2(loo_fit2)
loo_R2_fit3 <- loo_R2(loo_fit3)

# Display LOO R^2
print(loo_R2_fit0)
print(loo_R2_fit1)
print(loo_R2_fit2)
print(loo_R2_fit3)

# Compare models using LOO
loo_compare_models <- loo_compare(loo_fit0, loo_fit1, loo_fit2, loo_fit3)
print(loo_compare_models)

# Plot posterior distributions for Model 1 (standardized predictors)
mcmc_areas(as.matrix(fit1), prob = 0.95) +
  ggtitle("Posterior Distributions of Coefficients (Standardized Predictors)")

# Plot posterior distributions for Model 2 (scaled normal priors)
mcmc_areas(as.matrix(fit2), prob = 0.95) +
  ggtitle("Posterior Distributions of Coefficients (Scaled Normal Priors)")

# Plot posterior distributions for Model 3 (regularized horseshoe prior)
mcmc_areas(as.matrix(fit3), prob = 0.95) +
  ggtitle("Posterior Distributions of Coefficients (Regularized Horseshoe Prior)")

# Split the data into two halves
half_index <- floor(nrow(datastd_G3port) / 2)
data_shuffled <- datastd_G3port %>% sample_frac(1)  # Shuffle the data

first_half <- data_shuffled[1:half_index, ]
second_half <- data_shuffled[(half_index + 1):nrow(data_shuffled), ]

# Fit the best model (e.g., Model 3) on the first half
cv_model <- stan_glm(as.formula(paste(outcome, "~ .")),
                    data = first_half,
                    family = gaussian(),
                    prior = hs(global_scale = global_scale, slab_scale = slab_scale),
                    prior_intercept = normal(0, 5),
                    chains = 4,
                    iter = 2000,
                    seed = 123)

# Predict on the second half
cv_predictions_log <- predict(cv_model, newdata = second_half)

# If the model was fitted on standardized data, reverse the transformation
# Since outcome was not standardized, predictions are on the original scale

# Actual outcomes
actual_mort <- second_half[[outcome]]

# Calculate prediction errors
mae <- mean(abs(cv_predictions_log - actual_mort))
rmse <- sqrt(mean((cv_predictions_log - actual_mort)^2))

# Display prediction results and error metrics
cat("Cross-Validation Predictions (Model 3):\n")
print(data.frame(Actual = actual_mort, Predicted = cv_predictions_log))

cat(sprintf("\nPrediction Error Metrics:\nMean Absolute Error (MAE): %.2f\nRoot Mean Squared Error (RMSE): %.2f\n", mae, rmse))

# Optional: Plot Actual vs Predicted
ggplot(data.frame(Actual = actual_mort, Predicted = cv_predictions_log), aes(x = Actual, y = Predicted)) +
  geom_point(color = "steelblue") +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red") +
  labs(title = "Actual vs Predicted Portuguese Grades",
       x = "Actual Grade",
       y = "Predicted Grade") +
  theme_minimal()

cat("\nSummary of Analyses:\n")
cat("1. **Model 0** (Unstandardized Predictors): Bayesian R² =", round(median(bayes_R2_fit0$Estimate), 3),
    ", LOO R² =", round(median(loo_R2_fit0$Estimate), 3), "\n")
cat("2. **Model 1** (Standardized Predictors): Bayesian R² =", round(median(bayes_R2_fit1$Estimate), 3),
    ", LOO R² =", round(median(loo_R2_fit1$Estimate), 3), "\n")
cat("3. **Model 2** (Scaled Normal Priors): Bayesian R² =", round(median(bayes_R2_fit2$Estimate), 3),
    ", LOO R² =", round(median(loo_R2_fit2$Estimate), 3), "\n")
cat("4. **Model 3** (Regularized Horseshoe Prior): Bayesian R² =", round(median(bayes_R2_fit3$Estimate), 3),
    ", LOO R² =", round(median(loo_R2_fit3$Estimate), 3), "\n\n")

cat("**Model Comparison:**\n")
print(loo_compare_models)

cat("\n**Cross-Validation Error Metrics for Model 3:**\n")
cat(sprintf("Mean Absolute Error (MAE): %.2f\n", mae))
cat(sprintf("Root Mean Squared Error (RMSE): %.2f\n", rmse))

cat("\n**Interpretations:**\n")
cat("- **Model 0** shows initial fit without standardizing predictors. The Bayesian R² and LOO R² indicate the proportion of variance explained by the model.\n")
cat("- **Model 1** improves interpretability by standardizing predictors, ensuring all have equal variance.\n")
cat("- **Model 2** introduces scaled normal priors based on the number of predictors, aiming to control overfitting.\n")
cat("- **Model 3** employs a regularized horseshoe prior, effectively shrinking less relevant coefficients towards zero and highlighting the most important predictors.\n")
cat("- **Cross-Validation** demonstrates the predictive performance of the best model (Model 3) on unseen data, with MAE and RMSE providing quantitative measures of error.\n")

```

