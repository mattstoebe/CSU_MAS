---
title: "551_Case_study"
author: "Matthew Stoebe"
date: "`r Sys.Date()`"
output: html_document
---
#Setup
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

install.packages("car")

library(corrplot)
library(car)
library(psych)
library(tidyverse)
library(loo)
library(boot)
```

#EDA
```{r}

# Read the CSV file
lpga_data <- read.csv("LPGA.csv")

# View the first few rows
head(lpga_data)

# Check data structure
str(lpga_data)

# Convert percentage columns to numeric if they are not already
lpga_data$pctfrwy <- as.numeric(lpga_data$pctfrwy)
lpga_data$pctgrn <- as.numeric(lpga_data$pctgrn)

# Summary of quantitative variables
summary(lpga_data)

# Detailed descriptive statistics
library(psych)
describe(lpga_data[, sapply(lpga_data, is.numeric)])

# Histogram for each quantitative variable
lpga_data %>%
  select(-Golfer) %>%
  gather(key = "variable", value = "value") %>%
  ggplot(aes(x = value)) +
  geom_histogram(bins = 20, fill = "skyblue", color = "black") +
  facet_wrap(~ variable, scales = "free") +
  theme_minimal()

predictors <- names(lpga_data)[!(names(lpga_data) %in% c("Golfer", "przrnd"))]

for (var in predictors) {
  p <- ggplot(lpga_data, aes(x = .data[[var]], y = przrnd)) +
    geom_point(color = "blue") +
    geom_smooth(method = "lm", se = FALSE, color = "red") +
    labs(title = paste("przrnd vs", var), x = var, y = "Prize Money per Round") +
    theme_minimal()
  
  print(p)
}
# Correlation matrix of quantitative variables
numeric_vars <- lpga_data %>% select(-Golfer)
cor_matrix <- cor(numeric_vars)

corrplot(cor_matrix, method = "ellipse")


```

#Basic Model
```{r}
# Fit a multiple linear regression model with all predictors
initial_model <- lm(przrnd ~ . - Golfer, data = lpga_data)

# Summary of the model
summary(initial_model)

# Plot residuals to check assumptions
par(mfrow = c(2, 2))
plot(initial_model)
par(mfrow = c(1, 1))  # Reset plotting area

# Q-Q plot
qqnorm(residuals(initial_model))
qqline(residuals(initial_model), col = "red")

```

#Log Transform on Response
```{r}
log_response <- lm(log(przrnd) ~ . - Golfer, data = lpga_data)

# Summary of the model
summary(log_response)

# Plot residuals to check assumptions
par(mfrow = c(2, 2))
plot(log_response)
par(mfrow = c(1, 1))  # Reset plotting area

# Q-Q plot
qqnorm(residuals(log_response))
qqline(residuals(log_response), col = "red")


```
#Reduced Feature Set
```{r}
# Fit a linear model with selected predictors
model_reduced <- lm(log(przrnd) ~ rounds + pctgrn + aveputt, data = lpga_data)

# Check the model summary
summary(model_reduced)

# Diagnostic plots to evaluate the reduced model fit
par(mfrow = c(2, 2))
plot(model_reduced)

```
```{r}
# Load the necessary packages

```

#Model Comparison

```{r}


# Also, display the adjusted R-squared for comparison
initial_adj_r2 <- summary(initial_model)$adj.r.squared
log_response_adj_r2 <- summary(log_response)$adj.r.squared
reduced_adj_r2 <- summary(model_reduced)$adj.r.squared

cat("Initial Model Adjusted R-squared:", initial_adj_r2, "\n")
cat("Log-Transformed Model Adjusted R-squared:", log_response_adj_r2, "\n")
cat("Reduced Model Adjusted R-squared:", reduced_adj_r2, "\n")

```

# Standardize features
```{r}
# Load necessary libraries
library(tidyverse)

# Read the CSV file
lpga_data <- read.csv("LPGA.csv")

# Remove 'Golfer' and 'przrnd' from predictors, as 'Golfer' is categorical and 'przrnd' is the target
predictors <- names(lpga_data)[!(names(lpga_data) %in% c("Golfer", "przrnd"))]

# Standardize the predictor variables
lpga_data_standardized <- lpga_data %>%
  mutate(across(all_of(predictors), ~ scale(.) %>% as.vector))

# Fit a linear model to predict log(przrnd) using standardized inputs
model_standardized <- lm(log(przrnd) ~ . - Golfer, data = lpga_data_standardized)

# Summary of the standardized model
summary(model_standardized)

# Use the model to predict log earnings (log(przrnd))
lpga_data_standardized$log_przrnd_pred <- predict(model_standardized, newdata = lpga_data_standardized)

# View the first few rows of the data with predictions
head(lpga_data_standardized)
```
```{r}
library("rprojroot")
library("rstantools")
library("rstanarm")
library("loo")
library("ggplot2")
library("bayesplot")

# Fit the model using stan_glm
fit1 <- stan_glm(log(przrnd) ~ . - Golfer, data = lpga_data_standardized, refresh = 0)

# Plot the posterior distributions of the coefficients
p1 <- mcmc_areas(as.matrix(fit1), pars = vars(-'(Intercept)', -sigma),
                 prob_outer = 0.95, area_method = "scaled height")# +
#  xlim(c(-1.2, 0.8))

# Reverse the y-axis labels to match order of parameters
p1 <- p1 + scale_y_discrete(limits = rev(levels(p1$data$parameter)))

# Show the plot
p1
```

```{r}
bayes_r2_vals <- bayes_R2(fit1)

# Plot histogram of Bayesian R^2
ggplot() + 
  geom_histogram(aes(x = bayes_r2_vals), breaks = seq(0, 1, length.out = 100)) +
  xlim(c(0, 1)) +
  scale_y_continuous(breaks = NULL) +
  labs(x = "Bayesian R^2", y = "")

```

```{r}
library(glmnet)

# Prepare the data (excluding 'Golfer' and 'przrnd' for predictors, using log(przrnd) as the response)
lpga_data <- read.csv("LPGA.csv")

# Standardize predictors (except 'Golfer')
predictors <- names(lpga_data)[!(names(lpga_data) %in% c("Golfer", "przrnd"))]

# Convert to matrix for glmnet
x <- as.matrix(lpga_data[, predictors])
y <- log(lpga_data$przrnd)  # Log transform the response variable

# Split the data into training and testing sets
set.seed(123)
train_idx <- sample(1:nrow(lpga_data), size = 0.8 * nrow(lpga_data))
x_train <- x[train_idx, ]
y_train <- y[train_idx]
x_test <- x[-train_idx, ]
y_test <- y[-train_idx]

# Perform cross-validation for Lasso to find the best lambda
cv_lasso <- cv.glmnet(x_train, y_train, alpha = 1)

# Lasso Model using the best lambda from cross-validation
lasso_model <- glmnet(x_train, y_train, alpha = 1, lambda = cv_lasso$lambda.min)

# Coefficients of the Lasso model
lasso_coef <- coef(lasso_model)

# Display the selected features (non-zero coefficients)
selected_features_lasso <- lasso_coef[lasso_coef != 0]
print("Selected features by Lasso (non-zero coefficients):")
print(selected_features_lasso)

# Predict on test data using Lasso
lasso_pred <- predict(lasso_model, newx = x_test)

# Evaluate model performance using performance metrics
# Calculate RMSE (Root Mean Squared Error)
lasso_rmse <- sqrt(mean((y_test - lasso_pred)^2))

# Calculate R-squared
sst <- sum((y_test - mean(y_test))^2)
sse <- sum((y_test - lasso_pred)^2)
lasso_r2 <- 1 - (sse / sst)

# Print performance metrics
cat("Lasso RMSE:", lasso_rmse, "\n")
cat("Lasso R-squared:", lasso_r2, "\n")




```

