{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1\n",
    "\n",
    "For the Reuter data that we analyzed in class, we did not compare the deep learning results\n",
    "with any baseline. Please compare the results with Multiclass Logistic Regression, which can\n",
    "serve as a baseline. Report your findings and submit the Python code as an appendix. (You can\n",
    "fit a multiclass logistic regression by modifying the code for Reuter data slightly.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Logistic Regression\n",
      "  Test accuracy:    0.7925\n",
      "  Macro F1-score:   0.5666\n",
      "  Weighted F1-score:0.7816\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from keras.datasets import reuters\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "(num_words,) = (10000,)\n",
    "(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=num_words)\n",
    "\n",
    "def vectorize_sequences(sequences, dimension=num_words):\n",
    "    results = np.zeros((len(sequences), dimension), dtype='float32')\n",
    "    for i, seq in enumerate(sequences):\n",
    "        results[i, seq] = 1.0\n",
    "    return results\n",
    "\n",
    "X_train = vectorize_sequences(train_data)\n",
    "X_test  = vectorize_sequences(test_data)\n",
    "y_train = train_labels\n",
    "y_test  = test_labels\n",
    "\n",
    "logreg = LogisticRegression(\n",
    "    solver='lbfgs',\n",
    "    max_iter=1000,\n",
    "    n_jobs=-1\n",
    ")\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "y_pred = logreg.predict(X_test)\n",
    "acc      = accuracy_score(y_test, y_pred)\n",
    "f1_macro = f1_score(y_test, y_pred, average='macro')\n",
    "f1_weighted = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(f\"Baseline Logistic Regression\")\n",
    "print(f\"  Test accuracy:    {acc:.4f}\")\n",
    "print(f\"  Macro F1-score:   {f1_macro:.4f}\")\n",
    "print(f\"  Weighted F1-score:{f1_weighted:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: The deep learning model outperforms the simple multinomial logistic regression by roughly 23 percentage points in overall accuracy, demonstrating that the network‚Äôs hidden representations capture topic distinctions that the linear baseline cannot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2\n",
    "\n",
    "In this problem, you will analyze an SMS spam dataset collected from\n",
    "https://dl.acm.org/doi/10.1145/2034691.2034742. The dataset contains both legitimate SMS\n",
    "messages (labeled as ‚Äúham‚Äù) and spam messages (labeled as ‚Äúspam‚Äù). I have prepared a data\n",
    "matrix where the (ùëñ, ùëó)-th element represents the count of the j-th word in the i-th message. I\n",
    "have included words that appear in at least 1% of the messages. Your task is to build a deep\n",
    "neural network model to classify ‚Äúham‚Äù and ‚Äúspam‚Äù messages. Write a short report about your\n",
    "analysis. Consider the following questions: Have you compared the results with random\n",
    "guessing or baselines? Is the metric ‚Äúaccuracy‚Äù a good choice for this analysis? The dataset\n",
    "‚Äúmessages.csv‚Äù is attached."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
