---
title: "551 Assignment 2"
author: "Matthew Stoebe"
date: "`r Sys.Date()`"
output:
  word_document: default
  html_document: default
---

```{r setup, include=FALSE, echo=TRUE}
knitr::opts_chunk$set(echo = TRUE)
```

# Homework Questions:

##Chapter 4: 
###4.10

Survey weighting: Compare two options for a national opinion survey: (a) a simple random
sample of 1000 Americans, or (b) a survey that oversamples Latinos, with 300 randomly sampled
Latinos and 700 others randomly sampled from the non-Latino population. One of these options
will give more accurate comparisons between Latinos and others; the other will give more
accurate estimates for the total population average.

(a) Which option gives more accurate comparisons and which option gives more accurate
population estimates?

(b) Explain your answer above by computing standard errors for the Latino/other comparison and
the national average under each design. Assume that the national population is 15% Latino,
that the items of interest are yes/no questions with approximately equal proportions of each
response, and (unrealistically) that the surveys have no problems with nonresponse.

####Code
```{r}

#parameters
p_yes <- 0.5          
n_total <- 1000       

# Option (a) - Simple Random Sample
n_latino_a <- 150     
n_nonlatino_a <- 850 

# Option (b) - Oversampling Latinos
n_latino_b <- 300     
n_nonlatino_b <- 700  

# Function to calculate the standard error for the comparison between two groups
standard_error_comparison <- function(p1, p2, n1, n2) {
  sqrt((p1 * (1 - p1)) / n1 + (p2 * (1 - p2)) / n2)
}

# Function to calculate the standard error for the total population
standard_error_total <- function(p, n) {
  sqrt((p * (1 - p)) / n)
}

# Standard errors for Latino vs Non-Latino comparison in both options
se_comparison_a <- standard_error_comparison(p1 = p_yes, p2 = p_yes, n1 = n_latino_a, n2 = n_nonlatino_a)
se_comparison_b <- standard_error_comparison(p1 = p_yes, p2 = p_yes, n1 = n_latino_b, n2 = n_nonlatino_b)

# Standard errors for the total population in both options (same for both since n_total = 1000)
se_total_a <- standard_error_total(p = p_yes, n = n_total)
se_total_b <- standard_error_total(p = p_yes, n = n_total)

cat("Option (a) - Simple Random Sample:\n")
cat("SE (Latino vs Non-Latino comparison):", round(se_comparison_a, 4), "\n")
cat("SE (Total Population):", round(se_total_a, 4), "\n\n")

cat("Option (b) - Oversampling Latinos:\n")
cat("SE (Latino vs Non-Latino comparison):", round(se_comparison_b, 4), "\n")
cat("SE (Total Population):", round(se_total_b, 4), "\n")


```
####Answer
For the national opinion survey, Option (a) (simple random sample of 1000 Americans) provides a more accurate estimate for the total population average with a standard error of 0.0158. However, Option (b) (oversampling Latinos with 300 Latinos and 700 non-Latinos) offers more accurate comparisons between Latinos and non-Latinos, with a smaller standard error of 0.034 compared to 0.043 in Option (a).

The difference arises because Option (b) increases the Latino sample size, reducing the standard error for group comparisons. However, for overall population estimates, both options have the same total sample size, so their standard errors are equal for that purpose.


##Chapter 5: 
###5.2, 

Continuous probability simulation: The logarithms of weights (in pounds) of men in the United
States are approximately normally distributed with mean 5.13 and standard deviation 0.17;
women’s log weights are approximately normally distributed with mean 4.96 and standard
deviation 0.20. Suppose 10 adults selected at random step on an elevator with a capacity of 1750
pounds. What is the probability that their total weight exceeds this limit?

####Code

```{r}
mean_log_men <- 5.13
sd_log_men <- .17
mean_log_women <- 4.96
sd_log_women <- .20

weight_limit <- 1750

n_individuals <-10

# Proportion of men and women (assume 50/50 split)
prop_men <- 0.5
n_men <- round(n_individuals * prop_men)
n_women <- n_individuals - n_men

n_sim <- 10000

exceeds_limit <- replicate(n_sim, {
  
  #rlnorm to avoid having to transform from log
  men_weights <- rlnorm(n_men, meanlog = mean_log_men, sdlog = sd_log_men)
  women_weights <- rlnorm(n_women, meanlog = mean_log_women, sdlog = sd_log_women)
  
  # Calculate the total weight
  total_weight <- sum(men_weights) + sum(women_weights)
  
  # Check if the total weight exceeds the elevator limit
  return(total_weight > weight_limit)
})

prob_exceed <- mean(exceeds_limit)

# Print the result
cat("Estimated probability that the total weight exceeds the elevator's capacity:", prob_exceed, "\n")

```

###5.6, 
Propagation of uncertainty: We use a highly idealized setting to illustrate the use of simulations
in combining uncertainties. Suppose a company changes its technology for widget production,
and a study estimates the cost savings at $5 per unit, but with a standard error of $4. Furthermore,
a forecast estimates the size of the market (that is, the number of widgets that will be sold)
at 40 000, with a standard error of 10 000. Assuming these two sources of uncertainty are
independent, use simulation to estimate the total amount of money saved by the new product
(that is, savings per unit, multiplied by size of the market).

####Code

```{r}
mean_savings_per_unit <- 5
sd_savings_per_unit <- 4
mean_market_size <- 40000
sd_market_size <- 10000

# Number of simulations
n_sim <- 100000

total_savings <- replicate(n_sim, {
  
  # Simulate savings per unit and market size
  simulated_savings_per_unit <- rnorm(1, mean = mean_savings_per_unit, sd = sd_savings_per_unit)
  simulated_market_size <- rnorm(1, mean = mean_market_size, sd = sd_market_size)
  
  # Calculate total savings
  simulated_savings_per_unit * simulated_market_size
})

# Summary of the results
cat("Mean total savings:", mean(total_savings), "\n")
cat("Standard deviation of total savings:", sd(total_savings), "\n")

# Plot the distribution of total savings
hist(total_savings, breaks = 50, main = "Distribution of Total Savings", 
     xlab = "Total Savings", col = "blue", border = "white")

```


###5.10
Inference for a ratio of parameters: A (hypothetical) study compares the costs and effectiveness
of two different medical treatments.
• In the first part of the study, the difference in costs between treatments A and B is estimated
at $600 per patient, with a standard error of $400, based on a regression with 50 degrees of
freedom.
• In the second part of the study, the difference in effectiveness is estimated at 3.0 (on some
relevant measure), with a standard error of 1.0, based on a regression with 100 degrees of
freedom.
• For simplicity, assume that the data from the two parts of the study were collected independently.
Inference is desired for the incremental cost-effectiveness ratio: the difference between the
average costs of the two treatments, divided by the difference between their average effectiveness,
a problem discussed further by Heitjan, Moskowitz, and Whang (1999).
(a) Create 1000 simulation draws of the cost difference and the effectiveness difference, and make
a scatterplot of these draws.
(b) Use simulation to come up with an estimate, 50% interval, and 95% interval for the incremental
cost-effectiveness ratio.
(c) Repeat, changing the standard error on the difference in effectiveness to 2.0.

####Code a
```{r}
# Parameters
mean_cost_diff <- 600
se_cost_diff <- 400
df_cost <- 50  # degrees of freedom for cost difference

mean_effect_diff <- 3.0
se_effect_diff <- 1.0
df_effect <- 100  # degrees of freedom for effectiveness difference

n_sim <- 1000  # Number of simulations

# Simulate cost differences from a t-distribution
cost_diff_sim <- mean_cost_diff + rt(n_sim, df = df_cost) * se_cost_diff

# Simulate effectiveness differences from a t-distribution
effect_diff_sim <- mean_effect_diff + rt(n_sim, df = df_effect) * se_effect_diff

# (a) Scatterplot of cost differences vs. effectiveness differences
plot(cost_diff_sim, effect_diff_sim,
     xlab = "Cost Difference",
     ylab = "Effectiveness Difference",
     main = "Scatterplot of Simulated Cost and Effectiveness Differences",
     col = "blue", pch = 16)

icer_sim <- cost_diff_sim / effect_diff_sim
icer_filtered <- icer_sim[abs(icer_sim) < 1000]


hist(icer_filtered, 
     breaks = 50, 
     main = "Histogram of Incremental Cost-Effectiveness Ratio (ICER)", 
     xlab = "ICER", 
     col = "lightblue", 
     border = "white")

```
####Code b
```{r}

# Calculate the ICER from the simulated cost and effectiveness differences
icer_sim <- cost_diff_sim / effect_diff_sim

# Calculate the point estimate (mean) of ICER
mean_icer <- mean(icer_sim)

# Calculate the 50% confidence interval 
ci_50_lower <- quantile(icer_sim, 0.25)
ci_50_upper <- quantile(icer_sim, 0.75)

# Calculate the 95% confidence interval 
ci_95_lower <- quantile(icer_sim, 0.025)
ci_95_upper <- quantile(icer_sim, 0.975)

# Print results
cat("Estimated median ICER:", mean_icer, "\n")
cat("50% confidence interval for ICER:", ci_50_lower, "to", ci_50_upper, "\n")
cat("95% confidence interval for ICER:", ci_95_lower, "to", ci_95_upper, "\n")


```
####Code c
```{r}

# Parameters
mean_cost_diff <- 600
se_cost_diff <- 400
df_cost <- 50  # degrees of freedom for cost difference

mean_effect_diff <- 3.0
df_effect <- 100  # degrees of freedom for effectiveness difference

n_sim <- 1000  

se_effect_diff <- 2.0

# Simulate new effectiveness differences
effect_diff_sim <- mean_effect_diff + rt(n_sim, df = df_effect) * se_effect_diff

# Calculate the new ICER
icer_sim <- cost_diff_sim / effect_diff_sim

# Remove infinite and NaN values
icer_sim <- icer_sim[is.finite(icer_sim)]

# Calculate the median ICER
mean_icer <- mean(icer_sim)

# Calculate the 50% confidence interval
ci_50_lower <- quantile(icer_sim, 0.25)
ci_50_upper <- quantile(icer_sim, 0.75)

# Calculate the 95% confidence interval
ci_95_lower <- quantile(icer_sim, 0.025)
ci_95_upper <- quantile(icer_sim, 0.975)

# Print results
cat("With SE of effectiveness difference = 2.0\n")
cat("Estimated median ICER:", mean_icer, "\n")
cat("50% confidence interval for ICER:", ci_50_lower, "to", ci_50_upper, "\n")
cat("95% confidence interval for ICER:", ci_95_lower, "to", ci_95_upper, "\n")

# Remove extreme ICER values for plotting
icer_filtered_2 <- icer_sim[effect_diff_sim > 0 & abs(icer_sim) < 1000]

# (1) Scatterplot of cost differences vs. effectiveness differences with SE = 2.0
plot(cost_diff_sim, effect_diff_sim,
     xlab = "Cost Difference",
     ylab = "Effectiveness Difference",
     main = "Scatterplot of Cost vs. Effectiveness Differences (SE = 2.0)",
     col = "blue", pch = 16)

# (2) Histogram of the new ICER with SE = 2.0
hist(icer_filtered_2, 
     breaks = 50, 
     main = "Histogram of ICER (SE = 2.0)", 
     xlab = "ICER", 
     col = "lightblue", 
     border = "white")

```


##Chapter 6: 
###6.2, 
Programming fake-data simulation: Write an R function to: (i) simulate n data points from
the model, y = a + bx + error, with data points x uniformly sampled from the range (0, 100)
and with errors drawn independently from the normal distribution with mean 0 and standard
deviation σ; (ii) fit a linear regression to the simulated data; and (iii) make a scatterplot of the
data and fitted regression line. Your function should take as arguments, a, b, n, σ, and it should
return the data, print out the fitted regression, and make the plot. Check your function by trying
it out on some values of a, b, n, σ.

```{r}

simulate_regression <- function(a,b,n,sigma){
  x <- runif(n, min=0, max=100)
  
  error <- rnorm(n, mean=0,  sd = sigma)
  
  y <- a + b*x + error
  
  model <- lm(y~x)
  
  plot(x, y, main = "Scatterplot with Fitted Regression Line",
       xlab = "x", ylab = "y", pch = 19, col = "blue")
  abline(model, col = "red", lwd = 2)
  
  # Return the data as a data frame
  data <- data.frame(x = x, y = y)
 
   return(data)
}


simulate_regression(5,2,100,10)

simulate_regression(10,2,1000,30)




```

###6.3
Variation, uncertainty, and sample size: Repeat the example in Section 6.2, varying the number
of data points, n. What happens to the parameter estimates and uncertainties when you increase
the number of observations?
```{r}

simulate_regression_no_charts <- function(a,b,n,sigma){
  x <- runif(n, min=0, max=100)
  
  error <- rnorm(n, mean=0,  sd = sigma)
  
  y <- a + b*x + error
  
  model <- lm(y~x)
 # Commenting out charts so that i can run a better sensitivity test
  
  #plot(x, y, main = "Scatterplot with Fitted Regression Line",
  #   xlab = "x", ylab = "y", pch = 19, col = "blue")
  #abline(model, col = "red", lwd = 2)
  
  # Return the data as a data frame
  data <- data.frame(x = x, y = y)
 
   return(data)
}

#parameters
a <- 5
b <- 7
signma <- 10

#sample sizes
n_values <- round(seq(10,10010, length.out= 100))

results <- data.frame(
  n = integer(),
  est_intercept = numeric(),
  se_intercept = numeric(),
  est_slope = numeric(),
  se_slope = numeric()
)

for (n in n_values) {
  data <- simulate_regression_no_charts(a, b, n, signma)
  
  # Fit the model
  model <- lm(y ~ x, data = data)
  summary_model <- summary(model)
  
  # Extract estimates and standard errors
  est_intercept <- summary_model$coefficients["(Intercept)", "Estimate"]
  se_intercept <- summary_model$coefficients["(Intercept)", "Std. Error"]
  est_slope <- summary_model$coefficients["x", "Estimate"]
  se_slope <- summary_model$coefficients["x", "Std. Error"]
  
  # Store the results
  results <- rbind(
    results,
    data.frame(
      n = n,
      est_intercept = est_intercept,
      se_intercept = se_intercept,
      est_slope = est_slope,
      se_slope = se_slope
    )
  )
}

print(results)

plot(results$n, results$se_intercept, type = "l", col = "blue", lwd = 2,
     xlab = "Sample Size (n)", ylab = "Standard Error",
     main = "Standard Error of Intercept vs Sample Size")

grid()

plot(results$n, results$se_slope, type = "l", col = "red", lwd = 2,
     xlab = "Sample Size (n)", ylab = "Standard Error",
     main = "Standard Error of Slope vs Sample Size")

grid()



```

Answer: While the slope and intercept do increase or vary throughout the samples, the main difference is that the standard error for both go down dramatically as the number of samples goes up. that said there is a point of diminishing returns as standard error plateaus 

##Chapter 7: 

###7.2,
Fake-data simulation and regression: Simulate 100 data points from the linear model, y =
a + bx + error, with a = 5, b = 7, the values of x being sampled at random from a uniform
distribution on the range [0, 50], and errors that are normally distributed with mean 0 and standard
deviation 3.
(a) Fit a regression line to these data and display the output.
(b) Graph a scatterplot of the data and the regression line.
(c) Use the text function in R to add the formula of the fitted line to the graph.
```{r}

# Parameters
n <- 100
a <- 5
b <- 7
sigma <- 3

x <- runif(n, min = 0, max = 50)
error <- rnorm(n, mean = 0, sd = sigma)
y <- a + b * x + error

model <- lm(y ~ x)
summary_model <- summary(model)
print(summary_model)

plot(x, y, main = "Scatterplot with Fitted Regression Line",
     xlab = "x", ylab = "y", pch = 19, col = "red")
abline(model, col = "blue", lwd = 2)

coefficients <- coef(model)
formula_text <- paste0("y = ", round(coefficients[1], 2),
                       " + ", round(coefficients[2], 2), " * x")
# Position the text on the plot
text_x <- min(x) + 7
text_y <- max(y)- 10
text(text_x, text_y, labels = formula_text, col = "blue", cex = 1.2)


```

###7.6
Formulating comparisons as regression models: Take the election forecasting model and simplify
it by creating a binary predictor defined as x = 0 if income growth is less than 2% and x = 1 if
income growth is more than 2%.
(a) Compute the difference in incumbent party’s vote share on average, comparing those two
groups of elections, and determine the standard error for this difference.
(b) Regress incumbent party’s vote share on the binary predictor of income growth and check
that the resulting estimate and standard error are the same as above.

```{r}

election_data = read.table("ROS-Examples-master/ElectionsEconomy/data/hibbs.dat", header=TRUE)


election_data$x <- ifelse(election_data$growth > 2,1,0)

mean_vote_low <- mean(election_data$vote[election_data$x == 0], na.rm = TRUE)

mean_vote_high <- mean(election_data$vote[election_data$x == 1], na.rm=TRUE)

diff_means <- mean_vote_high - mean_vote_low

# Number of observations in each group
n_low <- sum(election_data$x == 0)
n_high <- sum(election_data$x == 1)


# Standard deviation for each group
sd_low <- sd(election_data$vote[election_data$x == 0], na.rm = TRUE)
sd_high <- sd(election_data$vote[election_data$x == 1], na.rm = TRUE)

# Standard error of the difference
se_diff <- sqrt((sd_low^2 / n_low) + (sd_high^2 / n_high))

cat("Difference in mean vote share:", diff_means, "\n")
cat("Standard error of the difference:", se_diff, "\n\n")


model <- lm(vote~x, data=election_data)

estimate = coef(model)['x']

se_estimate <- summary(model)$coefficients["x", "Std. Error"]

cat("Regression estimate for x:", estimate, "\n")
cat("Standard error of the estimate:", se_estimate, "\n\n")


```
