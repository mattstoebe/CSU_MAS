---
title: "STAA 552: HW 5"
author: "Matthew Stoebe"
output:
  pdf_document:
    toc: no
    toc_depth: '2'
    includes:
      in_header: preamble.tex
header-includes:
- \usepackage[default]{sourcesanspro}
- \usepackage[T1]{fontenc}
mainfont: SourceSansPro
---

```{r setup, include=FALSE}
#Retain this code chunk!!!
library(knitr)
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(message = FALSE)
#install.packages("ResourceSelection")
library(ResourceSelection)
library(ggplot2)
library(reshape2)
library(MASS)

```

See Canvas Calendar for due date.  
60 points total, 4 points per problem unless otherwise noted.  
Content for all questions is from section 07 or earlier. 
Add or delete code chunks as needed.  

# Breast Cancer (Q1 - Q9)

In this group of questions, we will be fitting logistic regression models using training data.  This data is available from Canvas as `BCTrain.csv`.  This includes data from n = 512 subjects.  This data is a subset of a larger data set that is publicly available from UCI Machine Learning Repository and Kaggle as “Breast Cancer Wisconsin (Diagnostic) Data Set”.
The columns include:

- id number (should NOT be used for model fitting)
- **diagnosis (0,1) should be used as the response for all analyses.**  Malignant tumors are represented with a value of 1; benign tumors are represented with a value of 0.
- 10 predictor variables that are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe characteristics of the cell nuclei present in the image.

The goal is to predict diagnosis (Y) based on the predictor variables (X’s).  


:::: {.taskbox data-latex=""}
## Q1 (2 pts)

Calculate pairwise (Pearson) correlations between all predictors.  Round your result to two decimal places.
::::

*****

```{r}
#Q1
data <- read.csv("Data/BCTrain.csv")

corr_data <- data[, -1]

coors <- round(cor(corr_data),2)
print(coors)

melted_coors <- melt(coors)

# Create heatmap
ggplot(data = melted_coors, aes(x = Var1, y = Var2, fill = value)) +
  geom_tile() +
  geom_text(aes(label = value), color = "black", size = 3) +  # Add coefficients as text
  scale_fill_gradient2(low = "blue", mid = "white", high = "red", midpoint = 0) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) +
  labs(title = "Correlation Heatmap", x = "Features", y = "Features", fill = "Correlation")

data <- data[, !(names(data) %in% c("perimeter", "area", "concave_points", "id"))]

```
*****

:::: {.taskbox data-latex=""}
## Q2 

**For all further questions**, we will **DROP perimeter, area and concave_points**.  Based on your results from the previous question, explain why this is reasonable.  Be brief, but specific.
::::

******
Response  
These features are highly correlated and should be dropped to avoid multi-colinearity in the model
******

:::: {.taskbox data-latex=""}
## Q3

Perform model selection using backwards elimination.  This should be done using p-values and with alpha = 0.05.  Show the coefficients table for your final model.  You do NOT need to show the intermediate output, only the final model.  

NOTE: For purposes of predictive modeling, backwards elimination would NOT be a common choice.  We are using this approach here for practice.  
::::

*****

```{r}
#Q3
#I initially used the step function here but it defaults to AIC. I dont see a function for P value so i will make my own 

backward_elimination_p <- function(model, threshold = 0.05) {
  model_data <- model$data

  while (TRUE) {
    #Get Maximum P value 
    model_summary <- summary(model)
    p_values <- coef(model_summary)[, "Pr(>|z|)"]
    p_values <- p_values[-1]
    max_p <- max(p_values, na.rm = TRUE)
    
#Remove Column from maximum P Value if it is above threshold
    if (max_p > threshold) {
      var_to_remove <- names(p_values)[which.max(p_values)]
      formula <- as.formula(
        paste("diagnosis ~", paste(setdiff(names(p_values), var_to_remove), collapse = " + "))
      )      
      model <- glm(formula, data = model_data, family = binomial)
    } 
    else {
      break
    }
  }
  return(model)
}

full_model <- glm(diagnosis ~ ., data = data, family = binomial)
final_model <- backward_elimination_p(full_model)
summary(final_model)



```

*****

:::: {.taskbox data-latex=""}
## Q4

Perform model selection using **AIC** all subsets selection.  Show the coefficients table for your final model.  You do NOT need to show the intermediate output, only the final model.  
::::

*****
```{r}
#Q4
step_model <- step(full_model, direction = "both", trace = 0)

summary(step_model)
```

*****

## Q5

Consider your results to the previous questions.

:::: {.taskbox data-latex=""}
### Q5A (2 pts)

Is it surprising that the selected models (from Q3 and Q4) are different? Briefly comment. 
::::

*****
Response
It is not surprising. Different methods yield different results, and the P value is rather simplistic as it does not consider the number of parameters in the model alongside the goodness of fit. AIC can favor a more complex model with a better fit. 
*****

:::: {.taskbox data-latex=""}
### Q5B (2pts)

Is it surprising that the selected model (from Q4) includes a predictor with p > 0.05?  Briefly comment.
::::

*****
Response
No. There are more complex things to consider than just p value in selecting parameters for a model. P Significance is not supreme in model building or parameter selection. You instead want to look at the incremental uplift you are getting with a parameter relative to other versions of the model. Again, AIC allows us to have a more robust and nuanced view in model selection that the P value elimination does not. 
*****

**Important Note:** For all further questions, we will use the AIC selected model from Q4. 

:::: {.taskbox data-latex=""}
## Q6

Calculate McFadden’s "pseudo" R2 value for this model.
::::

*****
R2 = 0.7994026 

```{r}
#Q6
final_model <- step_model

final_loglike <- logLik(final_model)
null_model <- glm(diagnosis ~ 1, data=data, family=binomial)
null_loglike <- logLik(null_model)

r2_ish <- 1- (final_loglike/null_loglike)
r2_ish

```

*****

:::: {.taskbox data-latex=""}
## Q7

Run the Hosmer-Lemeshow test to test for lack of fit (using g = 10).   Using alpha = 0.05, give a conclusion in context. 
::::

*****

Response

```{r}
#Q7
hl_test <- hoslem.test(final_model$fitted.values, data$diagnosis,)

hl_test
```

*****

:::: {.taskbox data-latex=""}
## Q8 (6 pts)

Suppose that a cancer is classified as malignant if $\hat p > 0.5$ and benign if $\hat p \le 0.5$  Calculate the accuracy, TPR (true positive rate = sensitivity), TNR (true negative rate = specificity) for this model (based on the original, **training** data).   
::::

*****
Accuracy:   
TPR:   
TNR:  

```{r}
#Q8
pred_probs <- predict(final_model, type = "response")

pred_class <- ifelse(pred_probs > 0.5, 1, 0)

conf_matrix <- table(Predicted = pred_class, Actual = data$diagnosis)

# Calculate accuracy, TPR, and TNR
accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)
tpr <- conf_matrix[2, 2] / sum(conf_matrix[2, ])
tnr <- conf_matrix[1, 1] / sum(conf_matrix[1, ])

accuracy
tpr
tnr

```

*****

:::: {.taskbox data-latex=""}
## Q9 (6 ps)

Now use the **test** data (`BDTest.csv`) to calculate the accuracy, TPR (true positive rate = sensitivity), TNR (true negative rate = specificity) for this model.  
::::

*****
Accuracy:    
TPR:   
TNR:    

```{r}
#Q9
# Q9
# Load the test data
test_data <- read.csv("Data/BCTest.csv")

# Get predicted probabilities for the test data
test_pred_probs <- predict(final_model, newdata = test_data, type = "response")

# Classify based on the threshold of 0.5
test_pred_class <- ifelse(test_pred_probs > 0.5, 1, 0)

# Confusion matrix for test data
test_conf_matrix <- table(Predicted = test_pred_class, Actual = test_data$diagnosis)

# Calculate accuracy, TPR, and TNR for test data
test_accuracy <- sum(diag(test_conf_matrix)) / sum(test_conf_matrix)
test_tpr <- test_conf_matrix[2, 2] / sum(test_conf_matrix[2, ])
test_tnr <- test_conf_matrix[1, 1] / sum(test_conf_matrix[1, ])

test_accuracy
test_tpr
test_tnr

```

*****

# Extinction (Q10 - Q16)

A study was conducted in the Krunnit Islands between 1949 and 1959.  In each of N = 18 islands, the "Species at Risk" give the number of bird species present in 1949 (corresponding to $n_i$ = number of "trials").  The "Extinctions" represent those not present in 1959 (corresponding to $y_i$ = number of "events"). This is grouped data.  See the file `Extinction.csv` on Canvas for the data. 

Our goal is to model probability of extinction as a function of island Area.  Hence **extinction** is the event of interest for all questions. 

## Q10 

We will start with some simple exploratory graphs.

:::: {.taskbox data-latex=""}
### Q10A (2 pts)

Construct boxplots for Area on both the original and log transformed scales. 
::::

```{r}
#Q10
data <- read.csv("Data/Extinction.csv")

# Boxplot for original Area
boxplot(data$Area, main = "Boxplot of Area (Original Scale)", ylab = "Area")

# Boxplot for log-transformed Area
boxplot(log(data$Area), main = "Boxplot of Area (Log Transformed)", ylab = "Log(Area)")

head(data)
```

:::: {.taskbox data-latex=""}
### Q10B (2 pts)

Construct a scatter plot of proportion extinct vs Area.
::::

```{r, out.width="70%"}
#Q11
data$proportion_extinct <- data$Extinctions / data$SpeciesAtRisk

# Scatter plot of proportion extinct vs Area
plot(data$Area, data$proportion_extinct, main = "Proportion Extinct vs Area", xlab = "Area", ylab = "Proportion Extinct", pch = 19)
```


## Extinction Model 1 (Q11 - Q12)

For this group of questions, we consider logistic regression with **Area** as the predictor.  

:::: {.taskbox data-latex=""}
## Q11 (2 pts)

Fit the model and show the `summary()` output.
::::

******

```{r}
#Q11
model1 <- glm(cbind(Extinctions, SpeciesAtRisk - Extinctions) ~ Area, family = binomial, data = data)

# Show the summary of the model
summary(model1)

```

*****

:::: {.taskbox data-latex=""}
## Q12

Conduct a deviance-based lack-of-fit test for Model 1.  Give the test statistic, df and p-value. **Using alpha = 0.10**, give a conclusion in context.  
::::

*****
X2 =   
df =     
p =   
Conclusion in context:    

```{r}
#Q12
null_model1 <- glm(cbind(Extinctions, SpeciesAtRisk - Extinctions) ~ 1, family = binomial, data = data)

# Deviance-based lack-of-fit test
deviance_statistic <- deviance(model1) - deviance(null_model1)
df <- length(coef(model1)) - 1  # degrees of freedom
p_value <- 1 - pchisq(deviance_statistic, df)

# Display the results
deviance_statistic
df
p_value
```
*****


## Extinction Model 2 (Q13 - Q15)

For this group of questions, we consider logistic regression with **log(Area)** as the predictor.

:::: {.taskbox data-latex=""}
## Q13 (2 pts)

Fit the model and show the `summary()` output.
::::

*****

```{r}
#Q13

```
*****

:::: {.taskbox data-latex=""}
## Q14 

Conduct a deviance-based lack-of-fit test for Model 2.  Give the test statistic, df and p-value. Using **alpha = 0.10**, give a conclusion in context.  
::::

*****
X2 =    
df =  
p =   
Conclusion in context:    


```{r}
#Q14

```
*****


:::: {.taskbox data-latex=""}
## Q15

Using Model 2, create a plot of Pearson residuals versus fitted values. Do the residuals indicate any evidence of overdispersion?
::::

*****

```{r, out.width="70%"}
#Q15

```

Discussion

*****

:::: {.taskbox data-latex=""}
## Q16 (2 pts)

Comparing Model 1 (using Area) and Model2 (using log(Area)), which model is preferred based on AIC?   
::::

*****
Response

*****

# Appendix
```{r show-code, ref.label = all_labels(), echo = TRUE, eval = FALSE}
```